{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166c49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "device = torch.device(\"cuda:2\")\n",
    "#device = 'cpu'\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781b4506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Reverse</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love finding out who your true friends are. ...</td>\n",
       "      <td>(: best. the just seriously It's are. friends ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USER :/ bye</td>\n",
       "      <td>bye :/ USER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USER I absolutely love Kanye's voice USER</td>\n",
       "      <td>USER voice Kanye's love absolutely I USER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We're hiring a new Kiosk Supervisor! Think you...</td>\n",
       "      <td>Noon! at 7th, September is Deadline URL apply!...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Every time I see Dave Grohl with his long hipp...</td>\n",
       "      <td>heroin. on back he's if wonder I haircut hippy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  \\\n",
       "0  I love finding out who your true friends are. ...   \n",
       "1                                        USER :/ bye   \n",
       "2          USER I absolutely love Kanye's voice USER   \n",
       "3  We're hiring a new Kiosk Supervisor! Think you...   \n",
       "4  Every time I see Dave Grohl with his long hipp...   \n",
       "\n",
       "                                             Reverse  label  \n",
       "0  (: best. the just seriously It's are. friends ...      0  \n",
       "1                                        bye :/ USER      0  \n",
       "2          USER voice Kanye's love absolutely I USER      0  \n",
       "3  Noon! at 7th, September is Deadline URL apply!...      0  \n",
       "4  heroin. on back he's if wonder I haircut hippy...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Balanced.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db298b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Tweet', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b14a2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['Tweet'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.1, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3dfbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model1 = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14285e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model2 = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa4a92c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edb01b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2198: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 140,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 140,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 140,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1bde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d38f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33dc9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model1.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0be32799",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e7ea552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERT_Arch(nn.Module):\n",
    "\n",
    "#     def __init__(self, bert):\n",
    "      \n",
    "#       super(BERT_Arch, self).__init__()\n",
    "\n",
    "#       self.bert = bert \n",
    "      \n",
    "#       self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "#       self.relu =  nn.ReLU()\n",
    "\n",
    "#       self.fc1 = nn.Linear(768,512)\n",
    "     \n",
    "#       self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "#       self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "   \n",
    "#     def forward(self, sent_id, mask):\n",
    "#       #print(sent_id)\n",
    "#       #print(mask)\n",
    " \n",
    "#       _, cls_hs = self.bert(sent_id, mask)[:2]\n",
    "      \n",
    "#       x = self.fc1(cls_hs)\n",
    "\n",
    "#       x = self.relu(x)\n",
    "\n",
    "#       x = self.dropout(x)\n",
    "\n",
    "#       x = self.fc2(x)\n",
    "      \n",
    "#       x = self.softmax(x)\n",
    "#       print(x)\n",
    "\n",
    "#       return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "179d3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ROBERT_Arch(nn.Module):\n",
    "\n",
    "#     def __init__(self, robert):\n",
    "      \n",
    "#       super(ROBERT_Arch, self).__init__()\n",
    "\n",
    "#       self.robert = robert \n",
    "      \n",
    "#       self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "#       self.relu =  nn.ReLU()\n",
    "\n",
    "     \n",
    "#       self.fc1 = nn.Linear(768,2)\n",
    "      \n",
    "#       #self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "#       #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, sent_id, mask):\n",
    " \n",
    "#       _, cls_hs = self.robert(sent_id, mask)[:2]\n",
    "      \n",
    "#       x = self.fc1(cls_hs)\n",
    "\n",
    "#       x = self.relu(x)\n",
    "\n",
    "#       #x = self.dropout(x)\n",
    "\n",
    "#       #x = self.fc2(x)\n",
    "\n",
    "# #       x = self.softmax(x)\n",
    "#       print(x.shape)\n",
    "#       return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac0edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = BERT_Arch(model2)\n",
    "\n",
    "# model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecc9ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = ROBERT_Arch(model1)\n",
    "\n",
    "# model1 = model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a9c2f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): RobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c83f129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "\n",
    "    def __init__(self, model1, model2):\n",
    "      \n",
    "      super(Ensemble, self).__init__()\n",
    "\n",
    "      self.model1 = model1\n",
    "    \n",
    "      self.model2 = model2\n",
    "      \n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "     \n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    " \n",
    "      _, cls_hs1 = self.model1(sent_id, mask)[:2]\n",
    "      \n",
    "      #print('----------------------------------------------')\n",
    "    \n",
    "      _, cls_hs2 = self.model2(sent_id, mask)[:2]\n",
    "      \n",
    "      #print(cls_hs1.shape)\n",
    "      #print(cls_hs2.shape)\n",
    "        #cls_hs = np.mean(np.array([cls_hs1,cls_hs2]),axis=0)\n",
    "      #cls_hs = cls_hs1.add(cls_hs2)\n",
    "      #cls_hs = torch.mean(cls_hs)\n",
    "      cls_hs = (cls_hs1 + cls_hs2) / 2\n",
    "      #print(cls_hs)\n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      x = self.fc2(x)\n",
    "\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d1ce44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = Ensemble(model1,model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda42178",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ensemble.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cee866ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model1.parameters(),\n",
    "                  lr = 1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1858db93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1], y=1036    1\n",
      "1006    1\n",
      "1672    1\n",
      "90      0\n",
      "107     0\n",
      "       ..\n",
      "932     1\n",
      "1029    1\n",
      "854     0\n",
      "914     1\n",
      "1547    1\n",
      "Name: label, Length: 1440, dtype: int64 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8f62c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "weights = weights.to(device)\n",
    "\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74982178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "  ensemble.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  total_preds=[]\n",
    " \n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "    #print(sent_id)\n",
    "#     sent_id = sent_id + 0.0001\n",
    "#     mask = mask + 0.0001\n",
    "    #print(mask)\n",
    "    #print(mask * sent_id)\n",
    "    \n",
    "   \n",
    "    ensemble.zero_grad()        \n",
    "\n",
    "    preds = ensemble(sent_id, mask)\n",
    "    print(preds)\n",
    "\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(ensemble.parameters(), 1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  \n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98c99381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    " \n",
    "  ensemble.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  total_preds = []\n",
    "\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "            \n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "     \n",
    "      preds = ensemble(sent_id, mask)\n",
    "\n",
    "      \n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  \n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    " \n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d28667c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 2 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 3 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 4 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 5 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 6 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 7 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 8 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 9 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 10 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 11 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 12 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 13 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 14 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 15 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 16 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 17 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 18 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 19 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 20 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 21 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.700\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 22 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 23 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 24 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 25 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 26 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 27 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 28 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 29 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 30 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 31 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 32 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 33 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 34 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 35 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.700\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 36 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 37 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 38 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 39 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 40 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 41 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 42 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 43 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 44 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 45 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 46 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.698\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 47 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 48 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 49 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n",
      "\n",
      " Epoch 50 / 50\n",
      "  Batch    50  of    180.\n",
      "  Batch   100  of    180.\n",
      "  Batch   150  of    180.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.699\n",
      "Validation Loss: 0.698\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "   \n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "   \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(ensemble.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eedb0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  preds = ensemble(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2284de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67        90\n",
      "           1       0.00      0.00      0.00        90\n",
      "\n",
      "    accuracy                           0.50       180\n",
      "   macro avg       0.25      0.50      0.33       180\n",
      "weighted avg       0.25      0.50      0.33       180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43c29917",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "fpr, tpr, _ = roc_curve(test_y, preds)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f460c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC/ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa1b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
